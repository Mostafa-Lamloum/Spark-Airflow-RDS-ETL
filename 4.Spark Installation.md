# Installing Apache Spark

Apache Spark is an open-source distributed computing system used for big data processing and analytics. This guide will walk you through the steps to install Apache Spark on your machine.

**Prerequisites**

Before you install Apache Spark, you need to make sure you have the following prerequisites:
    
   * Java Development Kit (JDK) 8 or higher
   * Python 3.x
   * Ubuntu user with root permissions

## Installation steps

1. Download Apache Spark

You can download the latest version of Apache Spark from the official website [Apache Spark](https://spark.apache.org/downloads.html)

2. Extract the downloaded Apache Spark archive to a desired location on your machine:
  ```
  tar -xzf spark-3.2.0-bin-hadoop3.2.tgz
  ```
3. Move the extracted directory to a desired location on your machine:
  ```
  mv spark-3.2.0-bin-hadoop3.2 /opt/spark
  ```
4. Set Enviroment variables, open .bashrc file:
  ```
  sudo nano /home/.bashrc
  ```
5. Add the path to your spark directory in your .bashrc file:
  ```
  export SPARK_HOME=/PATH/TO/SPARK/spark/spark-3.3.2-bin-hadoop3
  export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
  ```
> Enviroment variables are set in order to allow running .sh files in your spark directory without using sudo or being in the containing directory

6. Configure Apache Spark for standalone mode by navigating to the Spark configuration directory:   
  ```
  cd /opt/spark/conf
  ```
7. Create a copy of the template configuration file:
  ```
  cp spark-env.sh.template spark-env.sh
  ```
8. Edit the spark-env.sh file to set the environment variables for your Spark configuration. For example:
  ```
    export SPARK_WORKER_INSTANCES=2
    export SPARK_WORKER_MEMORY=4g
    export SPARK_EXECUTOR_MEMORY=4g
    export SPARK_DRIVER_MEMORY=4g
    export SPARK_MASTER_HOST=your_hostname_or_ip
    export SPARK_MASTER_WEBUI_PORT=8050 
  ```
> Changing the spark master webui default port (8080 to 8050) is an important step to avoid any conflicts with the default airflow webserver port 8080
 
9. Save and close the spark-env.sh file.

## Test Apache Spark

1. Start the Spark master:
  ```
  /opt/spark/sbin/start-master.sh
  ```

2. Check the Spark master web UI by opening a browser and navigating to http://<your_host>:8050.

3. Start the Spark worker:
  ```
  /opt/spark/sbin/start-worker.sh spark://<your_host>:7077
  ```
  
4. Check the Spark worker web UI by opening a browser and navigating to http://<your_worker_host>:8081.

*Now spark cluster is up and running*


## Conclusion

You have successfully installed and configured Apache Spark on your machine. You can now use Spark to process big data and ETLs.
