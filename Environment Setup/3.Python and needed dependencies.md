Installing Python, Pip, Virtualenv, Pyspark, Airflow, and Pyspark Providers

This guide will take you through the process of installing Python, Pip, Virtualenv, Pyspark, Airflow, and Pyspark Providers on your machine. 
These tools are essential for developing and running data engineering applications using Apache Spark and Airflow.
Prerequisites

   * Ubuntu 22.04 or higher installed
   * Java and JDK installed
   * MySQL and MySQL Workbench installed

## Installation steps

1. To install Python on your machine, run the following command in your terminal:
  ```
  sudo apt-get update
  sudo apt-get install python3
  ```

2. Pip is the package installer for Python. To install Pip, run the following command:
  ```
  sudo apt-get install python3-pip
  ```
3. Virtualenv is a tool used to create isolated Python environment. To install Virtualenv, run the following command:
  ```
  sudo apt-get install python3-venv
  ```    
4. To verify the virtualenv installation, run the following command in your terminal:
  ```
  virtualenv --version
  ```    
5. Create a Virtual environment in your desired directory:
  ```
  virtualenv venv
  ```    
6. Activate or Deactivate environment using this command:
  ```
  source venv/bin/activate
  ```    
8. Pyspark is the Python library used for Apache Spark programming. To install Pyspark within your active enviroment, run the following command:
  ```
  pip3 install pyspark
  ```
9. Airflow is a platform to programmatically author, schedule, and monitor workflows. To install Airflow within your active enviroment, run the following commands:
  ```
  sudo apt-get install libmysqlclient-dev
  sudo apt-get install libssl-dev
  sudo apt-get install libkrb5-dev
  pip3 install apache-airflow
  ```
   Initiate the database for airflow's metadata using:
  ``` 
  airflow init db
  ```
10. Run the scheduler and webserver in separate terminal windows to verify Airflow is running correctly:
  ```
  airflow scheduler
  ```
  ```
  airflow webserver
  ```
11. After verifying airflow is running return to the terminal and run the following command to create a user for the webserver:
  ```
  airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin
  ```
  Set your desired username and password then press `ENTER`
  
12. Pyspark Providers are additional libraries that provide extra functionalities to Pyspark. To install Pyspark Providers within your active enviroment, run the following command:
  ```
  pip3 install apache-airflow-providers-apache-spark
  ```
  
## Conclusion

After completing these steps, you should have Python, Pip, Virtualenv, Pyspark, Airflow, and Pyspark Providers installed on your machine.
You can now start developing data engineering applications using Apache Spark and Airflow.
